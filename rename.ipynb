{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe5213d-0ed4-45d0-89f8-c282319f5518",
   "metadata": {},
   "source": [
    "This works on datarmor: \n",
    "'''\n",
    "qsub -I -q mpi_1 -l walltime=1:00:00\n",
    "bash\n",
    "conda activate /home/datawork-lops-iaocea/conda-env/pangeo-fish_1222/\n",
    "cd /home/datawork-taos-s/test\n",
    "ipython\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb847b-2bcd-483b-91bf-3d772c61d371",
   "metadata": {},
   "source": [
    "## List kerchunk catalogue for intranet usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1756895-3cf0-4596-a0a0-185a92bc4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "names=!find /home/datawork-taos-s/intranet/kerchunk/ref-marc/ |grep json.zst"
   ]
  },
  {
   "cell_type": "raw",
   "id": "570abbb6-cca5-493e-99c7-2c06901ea941",
   "metadata": {},
   "source": [
    "storage_options_in= {\"compression\": \"zstd\"}\n",
    "\n",
    "url_in='file:////home/datawork-lops-iaocea/catalog/kerchunk/satellite/osisaf_public_l3_east_atlantic_west_indian_meteosat.json.zst'\n",
    "with fsspec.open(url_in, **(storage_options_in or {})) as f:\n",
    "         old = ujson.load(f)    \n",
    "fs=fsspec.filesystem(\"reference\", fo=old)\n",
    "refs = fs.references\n",
    "\n",
    "for k, v in refs.items():\n",
    "     if isinstance(v, list):\n",
    "            print(v[0])\n",
    "osisaf_public_l3_east_atlantic_west_indian_meteosat.json.zst\n",
    "file:///home/ref-osisaf-public/data/sst/l3c/east_atlantic_west_indian/meteosat/2010/111/\n",
    "\n",
    "\n",
    "osisaf_public_l3_nar_avhrr_noa_19.json.zst\n",
    "osisaf_public_l3_naravhrr_metop_a.json.zst\n",
    "osisaf_public_l3_west_atlantic_east_pacific_goes.json.zst\n",
    "\n",
    "Where is the path from https://data-???.ifremer.fr for osisaf??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3e451-4553-40c5-b076-37616178e470",
   "metadata": {},
   "source": [
    "## Create directories for publishing the kerchunk to internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cd7865a-174d-48b4-9dbf-2d8bb27bb36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "paths= [ os.path.dirname(name)   for name in names]\n",
    "paths=list(set(paths))\n",
    "def createpath(path):\n",
    "    newpath=path.replace('intranet', 'public')\n",
    "    return os.makedirs(newpath, exist_ok=True)\n",
    "createpath= [ createpath(path)   for path in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614fa54-ac13-4af7-8100-7a347ee06ccc",
   "metadata": {},
   "source": [
    "## Translate kerchunk catalogue for intranet access to https access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaef0c11-e816-4604-97e4-b2157ca2a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import ujson\n",
    "import dask\n",
    "\n",
    "\n",
    "\n",
    "def match_keys(mapping, value):\n",
    "    for k in mapping:\n",
    "        if k in value: \n",
    "            return k\n",
    "        \n",
    "    raise ValueError(f\"could not find {value} in mapping\") \n",
    "    \n",
    "def match_in_keys(mapping,value):\n",
    "    try:\n",
    "        match_keys(mapping,value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False    \n",
    "\n",
    "def rename_target(refs, renames):\n",
    "    #from kerchunk.utils import conslidate\n",
    "    print('in rename_target')\n",
    "    \"\"\"Utility to change URLs in a reference set in a predictable way\n",
    "\n",
    "    For reference sets including templates, this is more easily done by\n",
    "    using template overrides at access time; but rewriting the references\n",
    "    and saving a new file means not having to do that every time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    refs: dict\n",
    "        Reference set\n",
    "    renames: dict[str, str]\n",
    "        Mapping from the old URL (including protocol, if this is how they appear\n",
    "        in the original) to new URL\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict: the altered reference set, which can be saved\n",
    "    \"\"\"\n",
    "    fs = fsspec.filesystem(\"reference\", fo=refs)  # to produce normalised refs\n",
    "    refs = fs.references\n",
    "    out = {}\n",
    "    for k, v in refs.items():\n",
    "        if isinstance(v, list) and v[0] in renames:\n",
    "            out[k] = [renames[v[0]]] + v[1:]\n",
    "        elif isinstance(v, list) and match_in_keys(renames, v[0]) :\n",
    "            url = v[0]\n",
    "            print(url)\n",
    "            key = match_keys(renames, url)\n",
    "            new_url = url.replace(key, renames[key])\n",
    "            out[k] = [new_url] + v[1:]\n",
    "            #print(new_url)\n",
    "        else:\n",
    "            out[k] = v\n",
    "        #    print('boo')\n",
    "    return consolidate(out)\n",
    "\n",
    "def rename_target_files(\n",
    "    url_in, renames, url_out=None, storage_options_in=None, storage_options_out=None):\n",
    "    print('in rename_target_files')\n",
    "\n",
    "    \"\"\"Perform URL renames on a reference set - read and write from JSON\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url_in: str\n",
    "        Original JSON reference set\n",
    "    renames: dict\n",
    "        URL renamings to perform (see ``renate_target``)\n",
    "    url_out: str | None\n",
    "        Where to write to. If None, overwrites original\n",
    "    storage_options_in: dict | None\n",
    "        passed to fsspec for opening url_in\n",
    "    storage_options_out: dict | None\n",
    "        passed to fsspec for opening url_out. If None, storage_options_in is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    with fsspec.open(url_in, **(storage_options_in or {})) as f:\n",
    "        old = ujson.load(f)\n",
    "    new = rename_target(old, renames)\n",
    "    if url_out is None:\n",
    "        url_out = url_in\n",
    "    if storage_options_out is None:\n",
    "        storage_options_out = storage_options_in\n",
    "    with fsspec.open(url_out, mode=\"wt\", **(storage_options_out or {})) as f:\n",
    "        ujson.dump(new, f)\n",
    "\n",
    "def consolidate(refs):\n",
    "    \"\"\"Turn raw references into output\"\"\"\n",
    "    out = {}\n",
    "    for k, v in refs.items():\n",
    "        if isinstance(v, bytes):\n",
    "            try:\n",
    "                # easiest way to test if data is ascii\n",
    "                out[k] = v.decode(\"ascii\")\n",
    "            except UnicodeDecodeError:\n",
    "                out[k] = (b\"base64:\" + base64.b64encode(v)).decode()\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return {\"version\": 1, \"refs\": out}\n",
    "\n",
    "@dask.delayed\n",
    "def translate(name):\n",
    "    in_path='file:///home/datawork-taos-s/intranet/kerchunk/ref-marc/'\n",
    "    out_path='file:///home/datawork-taos-s/public/kerchunk/ref-marc/'\n",
    "    name=name.replace('/home/datawork-taos-s/intranet/kerchunk/ref-marc/','')\n",
    "    url_in = in_path+name\n",
    "    url_out = out_path+name\n",
    "\n",
    "    renames={'file:///home/ref-marc/':'https://data-dataref.ifremer.fr/marc/'} \n",
    "\n",
    "\n",
    "    storage_options_in= {\"compression\": \"zstd\"}\n",
    "    storage_options_out= {\"compression\": \"zstd\"}\n",
    "    return rename_target_files(\n",
    "        url_in, renames, url_out, storage_options_in=storage_options_in\n",
    "        , storage_options_out=storage_options_out ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d34b3d14-b169-402c-aacf-d493b08a165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "translated= [ translate(name)   for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ed2e4-f4d9-4b73-bdf3-7aa6813f75d4",
   "metadata": {},
   "source": [
    "## Start Dask workers to do parallel translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2db96c96-3c24-404a-bf25-925e11c3a00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datawork-lops-iaocea/conda-env/pangeo-fish_1222/lib/python3.10/site-packages/dask_jobqueue/core.py:255: FutureWarning: job_extra has been renamed to job_extra_directives. You are still using it (even if only set to []; please also check config files). If you did not set job_extra_directives yet, job_extra will be respected for now, but it will be removed in a future release. If you already set job_extra_directives, job_extra is ignored and you can remove it.\n",
      "  warnings.warn(warn, FutureWarning)\n",
      "/home/datawork-lops-iaocea/conda-env/pangeo-fish_1222/lib/python3.10/site-packages/dask_jobqueue/core.py:255: FutureWarning: job_extra has been renamed to job_extra_directives. You are still using it (even if only set to []; please also check config files). If you did not set job_extra_directives yet, job_extra will be respected for now, but it will be removed in a future release. If you already set job_extra_directives, job_extra is ignored and you can remove it.\n",
      "  warnings.warn(warn, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import dask_hpcconfig\n",
    "cluster = dask_hpcconfig.cluster(\"datarmor\")\n",
    "cluster.scale(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e1124c2-0f39-4177-ba7d-b6f2d77fdf1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33520dbd-1c3e-4b93-b5a3-8f6c1dbd09e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.4 s, sys: 252 ms, total: 1.66 s\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ok=dask.compute(*translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6c245a2-6f86-40e4-8bb5-b6b3e43c9eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./toto/test.json.zstd', './toto/test-Copy1.json.zstd']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d68efc2-ed81-4523-b214-52b0e07b55ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datawork-lops-iaocea/conda-env/pangeo-fish_1222/lib/python3.10/site-packages/dask_jobqueue/core.py:255: FutureWarning: job_extra has been renamed to job_extra_directives. You are still using it (even if only set to []; please also check config files). If you did not set job_extra_directives yet, job_extra will be respected for now, but it will be removed in a future release. If you already set job_extra_directives, job_extra is ignored and you can remove it.\n",
      "  warnings.warn(warn, FutureWarning)\n",
      "2023-01-23 16:11:28,903 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2305af-8af1-405c-8a23-fb210394febe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
